# USG Analytics (code-only, dataset-specific)

Introduction. This is a pipeline to find the best regression model, revised from a course project. You can run single files to reproduce the results for U.S. gold ETF (Adj Close) price prediction using the dataset from Kaggle: https://www.kaggle.com/datasets/sid321axn/gold-price-prediction-dataset

A reproducible, code-only project for analyzing gold (Adj Close) by reframing the forecasting task as a supervised regression problem (not an explicit AR model).  
The raw data filename intentionally remains data/raw/FINAL_USO.csv. All project code lives in code_usg/.

---------------------------------------------------------------------

usg-analytics/
├─ code_usg/          — reusable project code (features, modeling, plots, pipeline helpers)
├─ scripts/           — entry-point scripts you run (EDA, full analysis)
├─ conf/              — configuration files (e.g., params.yml: paths, features, CV, model params)
├─ data/              — project data (kept separate to protect raw files)
│  ├─ raw/            — original inputs (e.g., FINAL_USO.csv), do not modify
│  └─ processed/      — outputs produced by the pipeline (e.g., lagged_dataset.csv)
├─ reports/           — deliverables generated by runs
│  └─ figures/        — plots (heatmap, time-series, diagnostics)
├─ tests/             — pytest suite for config, features, models, and pipeline
├─ README.md          — project overview and how to run
├─ requirements.txt   — runtime dependencies
└─ requirements-dev.txt — dev/test tools (pytest, linters/formatters)

---------------------------------------------------------------------

Quickstart

1) Create and activate an environment

python -m venv .venv

Install dependencies:

python -m pip install --upgrade pip setuptools wheel
pip install -r requirements.txt -r requirements-dev.txt

2) Configure the pipeline

Edit conf/params.yml if needed:

data:
  input_csv: data/raw/FINAL_USO.csv
  date_column: Date
  parse_dates: true
  index_as_date: true
  trend_suffix: "_Trend"

features:
  target: "Adj Close"
  variables: [ "Adj Close", "SP_close", "DJ_close", "USDI_Price", "EU_Price",
               "GDX_Close", "SF_Price", "PLT_Price", "PLD_Price",
               "RHO_PRICE", "USO_Close", "OF_Price", "OS_Price" ]
  rolling_means: [7, 30]
  lag_all_predictors: true

cv:
  strategy: "kfold"   # or "timeseries" for chronological splits
  n_splits: 10
  shuffle: true
  random_state: 42

3) Run single files to reproduce

EDA deliverables:

PYTHONPATH=. python scripts/describe_and_plot.py --config conf/params.yml

Outputs:
- reports/descriptive_stats.csv
- reports/median_values.csv
- reports/figures/corr_heatmap.png
- reports/figures/target_ts.png
- reports/figures/predictors_ts.png

Modeling deliverables:

PYTHONPATH=. python scripts/run_analysis.py --config conf/params.yml

Outputs:
- reports/metrics.json
- reports/run_metadata.json
- reports/figures/pred_vs_actual.png
- reports/figures/residuals.png
- data/processed/lagged_dataset.csv

Tests:

PYTHONPATH=. pytest -q

---------------------------------------------------------------------

How we turn a time series into a simple regression problem

Goal: predict today’s Adj Close from yesterday’s information and a couple of smoothed targets—without fitting AR/ARIMA/ETS models.

Cleaning & feature choices:
1. Drop “trend” flags: remove columns ending with *_Trend (often noisy/redundant).
2. Select key market variables to control multicollinearity and keep the design compact: Adj Close (target), SP_close, DJ_close, USDI_Price, EU_Price, GDX_Close, SF_Price, PLT_Price, PLD_Price, RHO_PRICE, USO_Close, OF_Price, OS_Price.
3. Short/long smoothers of the target: rolling means on Adj Close (7-day & 30-day) to capture momentum.
4. Encode time by shifting predictors: shift all predictors by one day (*_prev). Each row then has yesterday’s predictors with today’s target, making standard regression feasible while preserving causality.
5. Drop initial NaNs from rolling/lagging.

---------------------------------------------------------------------

Models & why the ensemble helps

- Baseline: naive “today = yesterday” (Adj Close_prev).
- Linear: OLS (best subset) via exhaustive search; Lasso/Ridge with scaling & CV-tuned alpha.
- Nonlinear: Random Forest (configurable defaults).
- Ensemble: simple average of RF + best-subset OLS + Lasso + Ridge predictions.

Why averaging works: Averaging diverse models reduces variance and can improve accuracy when errors are not perfectly correlated—idiosyncratic mistakes cancel out. This is a classical result in ensemble learning. Reference: Hansen, L. K., & Salamon, P. (1990). Neural Network Ensembles. IEEE TPAMI, 12(10), 993–1001.

---------------------------------------------------------------------

What this project is (and isn’t)

- A simple investigation showing how to:
  1. Reframe a time-series task as supervised regression via one-day lags & rolling summaries, and
  2. Achieve modest gains using a plain averaging ensemble.
- It is not a production forecaster and does not provide predictive power suitable for trading or investing.
- Do not rely on these results for economic/financial decisions.

---------------------------------------------------------------------

Tuning & tips

- Faster runs: reduce features.variables or cv.n_splits; the exhaustive subset step is slowest (you can omit it).
- Absolute paths: if preferred, set data.input_csv to an absolute path (use forward slashes on Windows).
- Time-aware CV: use cv.strategy: timeseries for chronological splits.

---------------------------------------------------------------------

Troubleshooting

- ModuleNotFoundError: code_usg → Mark code_usg/ as Sources Root or set PYTHONPATH=..
- FileNotFoundError: FINAL_USO.csv → ensure the file exists at data/raw/FINAL_USO.csv (or update the config).
- Verbose subset selection output → set print_progress=False (or verbose=0) in the ExhaustiveFeatureSelector.

---------------------------------------------------------------------

License

Specify your license here (e.g., MIT).  
© You — for research/educational use only; not for financial decision-making.
